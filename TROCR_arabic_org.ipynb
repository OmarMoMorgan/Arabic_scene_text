{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8470273,"sourceType":"datasetVersion","datasetId":5050563}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! if [ ! $pip_done ]; then pip install -q transformers ;fi \n! if [ ! $pip_done ]; then pip install -q datasets jiwer ;fi \n! if [ ! $pip_done ]; then pip install -q sentencepiece ;fi \n\npip_done = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:26:22.998247Z","iopub.execute_input":"2024-05-22T09:26:22.999042Z","iopub.status.idle":"2024-05-22T09:27:02.405982Z","shell.execute_reply.started":"2024-05-22T09:26:22.999003Z","shell.execute_reply":"2024-05-22T09:27:02.404783Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/OmarMoMorgan/Arabic_scene_text.git","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:35:40.357742Z","iopub.execute_input":"2024-05-22T09:35:40.358138Z","iopub.status.idle":"2024-05-22T09:35:41.630785Z","shell.execute_reply.started":"2024-05-22T09:35:40.358105Z","shell.execute_reply":"2024-05-22T09:35:41.629633Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'Arabic_scene_text'...\nremote: Enumerating objects: 48, done.\u001b[K\nremote: Counting objects: 100% (48/48), done.\u001b[K\nremote: Compressing objects: 100% (39/39), done.\u001b[K\nremote: Total 48 (delta 10), reused 45 (delta 7), pack-reused 0\u001b[K\nUnpacking objects: 100% (48/48), 16.52 KiB | 1.27 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn , optim\n\n\nimport sys\nsys.path.append('/kaggle/working/Arabic_scene_text') #change this to the name of the repo\nfrom models import build_model\nfrom tools import EarlyStopping, tune_model\nfrom data import train_test_split_ , perPixel_mean_std, perChannel_mean_std, build_transforms","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:30:49.675422Z","iopub.execute_input":"2024-05-22T09:30:49.675804Z","iopub.status.idle":"2024-05-22T09:30:49.682458Z","shell.execute_reply.started":"2024-05-22T09:30:49.675769Z","shell.execute_reply":"2024-05-22T09:30:49.681503Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"root_dir = \"/kaggle/input/str-arabic-dataset/Arabic_words_train\"\ncolumn_names = ['image_path', 'text']\ndf = pd.read_csv(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/gt.txt\",names = column_names)\n\n\ntest_size = 0.2\ntrain_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n\ntest_df , val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:14.380560Z","iopub.execute_input":"2024-05-22T09:29:14.381628Z","iopub.status.idle":"2024-05-22T09:29:14.427690Z","shell.execute_reply.started":"2024-05-22T09:29:14.381593Z","shell.execute_reply":"2024-05-22T09:29:14.427006Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:16.598594Z","iopub.execute_input":"2024-05-22T09:29:16.599260Z","iopub.status.idle":"2024-05-22T09:29:16.603998Z","shell.execute_reply.started":"2024-05-22T09:29:16.599227Z","shell.execute_reply":"2024-05-22T09:29:16.603138Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df.shape, val_df,test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:19.712562Z","iopub.execute_input":"2024-05-22T09:29:19.713474Z","iopub.status.idle":"2024-05-22T09:29:19.725358Z","shell.execute_reply.started":"2024-05-22T09:29:19.713440Z","shell.execute_reply":"2024-05-22T09:29:19.724374Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((3350, 2),\n         image_path      text\n 0    word_1483.png         و\n 1    word_2517.png     الجلد\n 2    word_3703.png     مستوى\n 3    word_3683.png      طريق\n 4    word_2603.png  الرياضية\n ..             ...       ...\n 414   word_544.png    أسطورة\n 415  word_1921.png   القدمين\n 416   word_139.png    ومكتبة\n 417  word_1071.png       برج\n 418  word_1006.png       مصر\n \n [419 rows x 2 columns],\n (419, 2))"},"metadata":{}}]},{"cell_type":"markdown","source":"Making the data loader here ","metadata":{}},{"cell_type":"code","source":"class ArabicSTRDataset(Dataset):\n    def __init__(self, root_dir, df, processor, tokenizer, max_target_length):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.tokenizer = tokenizer\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get file name and text\n        file_name = self.df.iloc[idx]['image_path']\n        text = self.df.iloc[idx]['text']\n\n        # Prepare image (resize and normalize)\n        image_path = f\"{self.root_dir}/{file_name}\"\n        image = Image.open(image_path).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n\n        # Encode the text\n        labels = self.tokenizer(text, padding=\"max_length\", max_length=self.max_target_length, return_tensors=\"pt\").input_ids\n        labels = labels.squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": labels}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:24.173134Z","iopub.execute_input":"2024-05-22T09:29:24.173840Z","iopub.status.idle":"2024-05-22T09:29:24.182900Z","shell.execute_reply.started":"2024-05-22T09:29:24.173804Z","shell.execute_reply":"2024-05-22T09:29:24.181794Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:26.042692Z","iopub.execute_input":"2024-05-22T09:29:26.043531Z","iopub.status.idle":"2024-05-22T09:29:26.076868Z","shell.execute_reply.started":"2024-05-22T09:29:26.043499Z","shell.execute_reply":"2024-05-22T09:29:26.075796Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"Making the pretrained model using gpt 2 as tokenizer and processor and for the backbone of the network we are using vision transormer with Deit archietcures and pretrained weights ","metadata":{}},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-stage1\")\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\ntokenizer_ = processor.tokenizer\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:29.319636Z","iopub.execute_input":"2024-05-22T09:29:29.320563Z","iopub.status.idle":"2024-05-22T09:29:45.583552Z","shell.execute_reply.started":"2024-05-22T09:29:29.320530Z","shell.execute_reply":"2024-05-22T09:29:45.582614Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e93fbd01261d4230ba6458411d4da36d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4297984e514b319d1e3fc23017e43c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca826c03e66e45738f82efcdaaca9553"}},"metadata":{}},{"name":"stderr","text":"2024-05-22 09:29:35.477422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-22 09:29:35.477557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-22 09:29:35.618926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b1bac4e004e4777b90cdf97f651e431"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15b1fbc0cacc4367aa4e0eec2afe17b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f947525e9354782986830674a728a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31c9e7d047094c8f8755fb0136df064c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45c41c05d8d40a0b2185c98567ac4a2"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"VisionEncoderDecoderModel(\n  (encoder): DeiTModel(\n    (embeddings): DeiTEmbeddings(\n      (patch_embeddings): DeiTPatchEmbeddings(\n        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): DeiTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x DeiTLayer(\n          (attention): DeiTAttention(\n            (attention): DeiTSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): DeiTSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): DeiTIntermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DeiTOutput(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n    (pooler): DeiTPooler(\n      (dense): Linear(in_features=384, out_features=384, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-5): 6 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (activation_fn): ReLU()\n            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Paramters set for processor ","metadata":{}},{"cell_type":"code","source":"model.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 512\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:45.585288Z","iopub.execute_input":"2024-05-22T09:29:45.585582Z","iopub.status.idle":"2024-05-22T09:29:45.591743Z","shell.execute_reply.started":"2024-05-22T09:29:45.585558Z","shell.execute_reply":"2024-05-22T09:29:45.590740Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = ArabicSTRDataset(root_dir=root_dir,\n                           df=train_df,\n                           processor=processor,\n                           tokenizer=processor.tokenizer,\n                           max_target_length=100)\n\neval_dataset = ArabicSTRDataset(root_dir=root_dir,\n                           df=val_df,\n                           processor=processor,\n                           tokenizer=processor.tokenizer,\n                           max_target_length=100)\n\ntest_dataset = ArabicSTRDataset(root_dir=root_dir,\n                           df=test_df,\n                           processor=processor,\n                           tokenizer=processor.tokenizer,\n                           max_target_length=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:45.593037Z","iopub.execute_input":"2024-05-22T09:29:45.593388Z","iopub.status.idle":"2024-05-22T09:29:45.703378Z","shell.execute_reply.started":"2024-05-22T09:29:45.593356Z","shell.execute_reply":"2024-05-22T09:29:45.702523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:29:45.705085Z","iopub.execute_input":"2024-05-22T09:29:45.705364Z","iopub.status.idle":"2024-05-22T09:29:45.716600Z","shell.execute_reply.started":"2024-05-22T09:29:45.705341Z","shell.execute_reply":"2024-05-22T09:29:45.715746Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience = 5,factor = 0.1,verbose=True)\nearlystopping = EarlyStopping()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:31:38.274176Z","iopub.execute_input":"2024-05-22T09:31:38.274829Z","iopub.status.idle":"2024-05-22T09:31:38.283682Z","shell.execute_reply.started":"2024-05-22T09:31:38.274794Z","shell.execute_reply":"2024-05-22T09:31:38.282889Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"hist_ = tune_model(100,model,train_dataloader,eval_dataloader,\\\n               optimizer,device,tokenizer_ , scheduler,earlystopping=earlystopping)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:35:46.383969Z","iopub.execute_input":"2024-05-22T09:35:46.384365Z","iopub.status.idle":"2024-05-22T09:35:46.976807Z","shell.execute_reply.started":"2024-05-22T09:35:46.384333Z","shell.execute_reply":"2024-05-22T09:35:46.975556Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/419 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4961204621d43c4b785a623adb4055f"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist_ \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearlystopping\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/working/Arabic_scene_text/tools/trainer.py:142\u001b[0m, in \u001b[0;36mtune_model\u001b[0;34m(num_epochs, model, train_dataloader_, test_dataloader_, optimizer, device, tokenizer_, scheduler, earlystopping)\u001b[0m\n\u001b[1;32m    140\u001b[0m f\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 142\u001b[0m     lss,acc\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     test_lss,test_acc\u001b[38;5;241m=\u001b[39m validate_epoch(model, test_dataloader_, optimizer, device , tokenizer_)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/kaggle/working/Arabic_scene_text/tools/trainer.py:47\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataloader, optimizer, device, tokenizer_)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#if i % 100 == 0: print(f\"Loss: {loss.item()}\") \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 47\u001b[0m     cer \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#valid_cer += cer\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     cer_hist\u001b[38;5;241m.\u001b[39mappend(cer)\n","\u001b[0;31mTypeError\u001b[0m: compute_cer() got an unexpected keyword argument 'tokenizer_'"],"ename":"TypeError","evalue":"compute_cer() got an unexpected keyword argument 'tokenizer_'","output_type":"error"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:35:36.108268Z","iopub.execute_input":"2024-05-22T09:35:36.108972Z","iopub.status.idle":"2024-05-22T09:35:37.099679Z","shell.execute_reply.started":"2024-05-22T09:35:36.108940Z","shell.execute_reply":"2024-05-22T09:35:37.098538Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]}]}