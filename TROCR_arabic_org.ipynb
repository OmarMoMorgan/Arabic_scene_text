{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:26:22.999042Z","iopub.status.busy":"2024-05-22T09:26:22.998247Z","iopub.status.idle":"2024-05-22T09:27:02.405982Z","shell.execute_reply":"2024-05-22T09:27:02.404783Z","shell.execute_reply.started":"2024-05-22T09:26:22.999003Z"},"trusted":true},"outputs":[],"source":["! if [ ! $pip_done ]; then pip install -q transformers ;fi \n","! if [ ! $pip_done ]; then pip install -q datasets jiwer ;fi \n","! if [ ! $pip_done ]; then pip install -q sentencepiece ;fi \n","\n","pip_done = 1"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:35:40.358138Z","iopub.status.busy":"2024-05-22T09:35:40.357742Z","iopub.status.idle":"2024-05-22T09:35:41.630785Z","shell.execute_reply":"2024-05-22T09:35:41.629633Z","shell.execute_reply.started":"2024-05-22T09:35:40.358105Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Cloning into 'Arabic_scene_text'...\n","remote: Enumerating objects: 48, done.\u001b[K\n","remote: Counting objects: 100% (48/48), done.\u001b[K\n","remote: Compressing objects: 100% (39/39), done.\u001b[K\n","remote: Total 48 (delta 10), reused 45 (delta 7), pack-reused 0\u001b[K\n","Unpacking objects: 100% (48/48), 16.52 KiB | 1.27 MiB/s, done.\n"]}],"source":["!git clone https://github.com/OmarMoMorgan/Arabic_scene_text.git"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:30:49.675804Z","iopub.status.busy":"2024-05-22T09:30:49.675422Z","iopub.status.idle":"2024-05-22T09:30:49.682458Z","shell.execute_reply":"2024-05-22T09:30:49.681503Z","shell.execute_reply.started":"2024-05-22T09:30:49.675769Z"},"trusted":true},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AdamW\n","from sklearn.model_selection import train_test_split\n","from torch import nn , optim\n","\n","\n","import sys\n","sys.path.append('/kaggle/working/Arabic_scene_text') #change this to the name of the repo\n","from models import build_model , KAN\n","from tools import EarlyStopping, tune_model , generate_text_with_caption , validate_epoch\n","from data import train_test_split_ , perPixel_mean_std, perChannel_mean_std, build_transforms"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:14.381628Z","iopub.status.busy":"2024-05-22T09:29:14.380560Z","iopub.status.idle":"2024-05-22T09:29:14.427690Z","shell.execute_reply":"2024-05-22T09:29:14.427006Z","shell.execute_reply.started":"2024-05-22T09:29:14.381593Z"},"trusted":true},"outputs":[],"source":["root_dir = \"/kaggle/input/str-arabic-dataset/Arabic_words_train\"\n","column_names = ['image_path', 'text']\n","df = pd.read_csv(\"/kaggle/input/str-arabic-dataset/Arabic_words_train/gt.txt\",names = column_names)\n","\n","\n","test_size = 0.2\n","train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n","\n","test_df , val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:16.599260Z","iopub.status.busy":"2024-05-22T09:29:16.598594Z","iopub.status.idle":"2024-05-22T09:29:16.603998Z","shell.execute_reply":"2024-05-22T09:29:16.603138Z","shell.execute_reply.started":"2024-05-22T09:29:16.599227Z"},"trusted":true},"outputs":[],"source":["train_df.reset_index(drop=True, inplace=True)\n","val_df.reset_index(drop=True, inplace=True)\n","test_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:19.713474Z","iopub.status.busy":"2024-05-22T09:29:19.712562Z","iopub.status.idle":"2024-05-22T09:29:19.725358Z","shell.execute_reply":"2024-05-22T09:29:19.724374Z","shell.execute_reply.started":"2024-05-22T09:29:19.713440Z"},"trusted":true},"outputs":[{"data":{"text/plain":["((3350, 2),\n","         image_path      text\n"," 0    word_1483.png         و\n"," 1    word_2517.png     الجلد\n"," 2    word_3703.png     مستوى\n"," 3    word_3683.png      طريق\n"," 4    word_2603.png  الرياضية\n"," ..             ...       ...\n"," 414   word_544.png    أسطورة\n"," 415  word_1921.png   القدمين\n"," 416   word_139.png    ومكتبة\n"," 417  word_1071.png       برج\n"," 418  word_1006.png       مصر\n"," \n"," [419 rows x 2 columns],\n"," (419, 2))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_df.shape, val_df,test_df.shape"]},{"cell_type":"markdown","metadata":{},"source":["Making the data loader here "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:24.173840Z","iopub.status.busy":"2024-05-22T09:29:24.173134Z","iopub.status.idle":"2024-05-22T09:29:24.182900Z","shell.execute_reply":"2024-05-22T09:29:24.181794Z","shell.execute_reply.started":"2024-05-22T09:29:24.173804Z"},"trusted":true},"outputs":[],"source":["class ArabicSTRDataset(Dataset):\n","    def __init__(self, root_dir, df, processor, tokenizer, max_target_length):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.processor = processor\n","        self.tokenizer = tokenizer\n","        self.max_target_length = max_target_length\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # Get file name and text\n","        file_name = self.df.iloc[idx]['image_path']\n","        text = self.df.iloc[idx]['text']\n","\n","        # Prepare image (resize and normalize)\n","        image_path = f\"{self.root_dir}/{file_name}\"\n","        image = Image.open(image_path).convert(\"RGB\")\n","        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n","\n","        # Encode the text\n","        labels = self.tokenizer(text, padding=\"max_length\", max_length=self.max_target_length, return_tensors=\"pt\").input_ids\n","        labels = labels.squeeze()\n","        labels[labels == self.tokenizer.pad_token_id] = -100\n","\n","\n","        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": labels}\n","        return encoding"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:26.043531Z","iopub.status.busy":"2024-05-22T09:29:26.042692Z","iopub.status.idle":"2024-05-22T09:29:26.076868Z","shell.execute_reply":"2024-05-22T09:29:26.075796Z","shell.execute_reply.started":"2024-05-22T09:29:26.043499Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["Making the pretrained model using gpt 2 as tokenizer and processor and for the backbone of the network we are using vision transormer with Deit archietcures and pretrained weights "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:29.320563Z","iopub.status.busy":"2024-05-22T09:29:29.319636Z","iopub.status.idle":"2024-05-22T09:29:45.583552Z","shell.execute_reply":"2024-05-22T09:29:45.582614Z","shell.execute_reply.started":"2024-05-22T09:29:29.320530Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e93fbd01261d4230ba6458411d4da36d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d4297984e514b319d1e3fc23017e43c","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca826c03e66e45738f82efcdaaca9553","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-05-22 09:29:35.477422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-22 09:29:35.477557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-22 09:29:35.618926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b1bac4e004e4777b90cdf97f651e431","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15b1fbc0cacc4367aa4e0eec2afe17b5","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f947525e9354782986830674a728a28","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31c9e7d047094c8f8755fb0136df064c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d45c41c05d8d40a0b2185c98567ac4a2","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["VisionEncoderDecoderModel(\n","  (encoder): DeiTModel(\n","    (embeddings): DeiTEmbeddings(\n","      (patch_embeddings): DeiTPatchEmbeddings(\n","        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): DeiTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x DeiTLayer(\n","          (attention): DeiTAttention(\n","            (attention): DeiTSelfAttention(\n","              (query): Linear(in_features=384, out_features=384, bias=True)\n","              (key): Linear(in_features=384, out_features=384, bias=True)\n","              (value): Linear(in_features=384, out_features=384, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): DeiTSelfOutput(\n","              (dense): Linear(in_features=384, out_features=384, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): DeiTIntermediate(\n","            (dense): Linear(in_features=384, out_features=1536, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): DeiTOutput(\n","            (dense): Linear(in_features=1536, out_features=384, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n","    (pooler): DeiTPooler(\n","      (dense): Linear(in_features=384, out_features=384, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (decoder): TrOCRForCausalLM(\n","    (model): TrOCRDecoderWrapper(\n","      (decoder): TrOCRDecoder(\n","        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n","        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n","        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (layers): ModuleList(\n","          (0-5): 6 x TrOCRDecoderLayer(\n","            (self_attn): TrOCRAttention(\n","              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","            )\n","            (activation_fn): ReLU()\n","            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): TrOCRAttention(\n","              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n","              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n","              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n","  )\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-stage1\")\n","processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\n","tokenizer_ = processor.tokenizer\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["Paramters set for processor "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:45.585582Z","iopub.status.busy":"2024-05-22T09:29:45.585288Z","iopub.status.idle":"2024-05-22T09:29:45.591743Z","shell.execute_reply":"2024-05-22T09:29:45.590740Z","shell.execute_reply.started":"2024-05-22T09:29:45.585558Z"},"trusted":true},"outputs":[],"source":["model.config.eos_token_id = processor.tokenizer.sep_token_id\n","model.config.max_length = 512\n","model.config.early_stopping = True\n","model.config.no_repeat_ngram_size = 3\n","model.config.length_penalty = 2.0\n","model.config.num_beams = 4\n","model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n","model.config.pad_token_id = processor.tokenizer.pad_token_id\n","\n","batch_size = 8"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:45.593388Z","iopub.status.busy":"2024-05-22T09:29:45.593037Z","iopub.status.idle":"2024-05-22T09:29:45.703378Z","shell.execute_reply":"2024-05-22T09:29:45.702523Z","shell.execute_reply.started":"2024-05-22T09:29:45.593356Z"},"trusted":true},"outputs":[],"source":["train_dataset = ArabicSTRDataset(root_dir=root_dir,\n","                           df=train_df,\n","                           processor=processor,\n","                           tokenizer=processor.tokenizer,\n","                           max_target_length=100)\n","\n","eval_dataset = ArabicSTRDataset(root_dir=root_dir,\n","                           df=val_df,\n","                           processor=processor,\n","                           tokenizer=processor.tokenizer,\n","                           max_target_length=100)\n","\n","test_dataset = ArabicSTRDataset(root_dir=root_dir,\n","                           df=test_df,\n","                           processor=processor,\n","                           tokenizer=processor.tokenizer,\n","                           max_target_length=100)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:29:45.705364Z","iopub.status.busy":"2024-05-22T09:29:45.705085Z","iopub.status.idle":"2024-05-22T09:29:45.716600Z","shell.execute_reply":"2024-05-22T09:29:45.715746Z","shell.execute_reply.started":"2024-05-22T09:29:45.705341Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:31:38.274829Z","iopub.status.busy":"2024-05-22T09:31:38.274176Z","iopub.status.idle":"2024-05-22T09:31:38.283682Z","shell.execute_reply":"2024-05-22T09:31:38.282889Z","shell.execute_reply.started":"2024-05-22T09:31:38.274794Z"},"trusted":true},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr=5e-5)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience = 5,factor = 0.1,verbose=True)\n","earlystopping = EarlyStopping()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:35:46.384365Z","iopub.status.busy":"2024-05-22T09:35:46.383969Z","iopub.status.idle":"2024-05-22T09:35:46.976807Z","shell.execute_reply":"2024-05-22T09:35:46.975556Z","shell.execute_reply.started":"2024-05-22T09:35:46.384333Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4961204621d43c4b785a623adb4055f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/419 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"compute_cer() got an unexpected keyword argument 'tokenizer_'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist_ \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearlystopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearlystopping\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/working/Arabic_scene_text/tools/trainer.py:142\u001b[0m, in \u001b[0;36mtune_model\u001b[0;34m(num_epochs, model, train_dataloader_, test_dataloader_, optimizer, device, tokenizer_, scheduler, earlystopping)\u001b[0m\n\u001b[1;32m    140\u001b[0m f\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 142\u001b[0m     lss,acc\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     test_lss,test_acc\u001b[38;5;241m=\u001b[39m validate_epoch(model, test_dataloader_, optimizer, device , tokenizer_)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/kaggle/working/Arabic_scene_text/tools/trainer.py:47\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_dataloader, optimizer, device, tokenizer_)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#if i % 100 == 0: print(f\"Loss: {loss.item()}\") \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 47\u001b[0m     cer \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#valid_cer += cer\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     cer_hist\u001b[38;5;241m.\u001b[39mappend(cer)\n","\u001b[0;31mTypeError\u001b[0m: compute_cer() got an unexpected keyword argument 'tokenizer_'"]}],"source":["hist_ = tune_model(100,model,train_dataloader,eval_dataloader,\\\n","               optimizer,device,tokenizer_ , scheduler,earlystopping=earlystopping)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T09:35:36.108972Z","iopub.status.busy":"2024-05-22T09:35:36.108268Z","iopub.status.idle":"2024-05-22T09:35:37.099679Z","shell.execute_reply":"2024-05-22T09:35:37.098538Z","shell.execute_reply.started":"2024-05-22T09:35:36.108940Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["#!rm -rf /kaggle/working/*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1011.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1013.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1017.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_1019.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_4011.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_2211.png',processor,device,model)\n","generate_text_with_caption('/kaggle/input/str-arabic-dataset/Arabic_words_train/word_3311.png',processor,device,model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lss_test, CER_test = validate_epoch(model,test_dataloader,optimizer,device,tokenizer_)\n","print('lss for test dataset is: ' , lss_test , 'CER is' , CER_test)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5050563,"sourceId":8470273,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
